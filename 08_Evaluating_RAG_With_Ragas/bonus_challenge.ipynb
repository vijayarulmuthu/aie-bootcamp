{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c712077a",
   "metadata": {},
   "source": [
    "# **BONUS CHALLENGE: LangGraph RAG + RAGAS Evaluation**\n",
    "\n",
    "Complete pipeline to:\n",
    "1. Build a baseline RAG system using naive chunking\n",
    "2. Evaluate it with RAGAS\n",
    "3. Implement semantic chunking\n",
    "4. Compare evaluation metrics between both systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3cb9a",
   "metadata": {},
   "source": [
    "## **Step 1: Baseline LangGraph RAG with Naive Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b44b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Please enter your OpenAI API key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695c89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict, List\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "# Step 1: Load and preprocess documents\n",
    "path = \"data/\"\n",
    "loader = DirectoryLoader(path, glob=\"*.html\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"# of chunks: {len(chunks)}\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "qdrant = QdrantClient(\":memory:\")\n",
    "collection_name = \"naive_chunking\"\n",
    "\n",
    "qdrant.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=qdrant,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "_ = vectorstore.add_documents(documents=chunks)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "\n",
    "# Step 2: LangGraph State Definition\n",
    "class GraphState(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document]\n",
    "  response: str\n",
    "  \n",
    "# Step 3: Retrieval Node\n",
    "def retrieve(state):\n",
    "  retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs}\n",
    "\n",
    "# Step 4: Augmentation\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "\n",
    "# Step 5: Generation Node\n",
    "def generate(state):\n",
    "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "  messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
    "  response = llm.invoke(messages)\n",
    "  return {\"response\" : response.content}\n",
    "\n",
    "# Step 6: Build LangGraph\n",
    "graph_builder = StateGraph(GraphState).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9956b",
   "metadata": {},
   "source": [
    "## **Step 2: Baseline Evaluation using RAGAS Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5268b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1b2d41d45b4d2fbd769e587fb20788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f170330cb4d4421a72c174f1c26db91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c67ae6118f5485a963cf572d60531a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509c70736aba4cbc931e182ce628f296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6e66991fdb4eeaa48a85f0e93e8bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: Error code: 500 - {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1524ff9fb4b34769bff6f55af743197a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc1402f64a944218b1b7442db2e5a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513a8d73f60a48b69dc226561ef0f2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad7b812ea604268a27687f78427f674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generated.\n",
      "Executing RAG chain...\n",
      "RAG chain executed successfully.\n",
      "Executing RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0d2f7e7fc24da292bbc1618cf2ad83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[19]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 30000, Requested 1476. Please try again in 2.952s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[5]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29831, Requested 2495. Please try again in 4.652s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[18]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29870, Requested 2214. Please try again in 4.168s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[20]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 28824, Requested 2432. Please try again in 2.512s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[10]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29507, Requested 2517. Please try again in 4.047s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[23]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29189, Requested 2096. Please try again in 2.57s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[25]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29915, Requested 1711. Please try again in 3.252s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[9]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29383, Requested 2021. Please try again in 2.808s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[30]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29145, Requested 2407. Please try again in 3.104s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[28]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 28330, Requested 2350. Please try again in 1.36s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[29]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 27597, Requested 2656. Please try again in 505ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS Results using Naive Retrieval:\n",
      "{'faithfulness': 0.8681, 'answer_relevancy': 0.7868, 'context_precision': 0.5333, 'context_recall': 0.8701, 'answer_correctness': 0.7673}\n"
     ]
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import EvaluationDataset\n",
    "from ragas import evaluate, RunConfig\n",
    "from ragas.metrics import (\n",
    "    faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness\n",
    ")\n",
    "\n",
    "# Generate testset from source documents\n",
    "print(\"Generating dataset...\")\n",
    "generator_llm = LangchainLLMWrapper(langchain_llm=ChatOpenAI(model=\"gpt-4.1\"),)\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(embeddings=embeddings)\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)\n",
    "print(\"Dataset generated.\")\n",
    "\n",
    "\n",
    "# Run inference and update answers\n",
    "print(\"Executing RAG chain...\")\n",
    "for test_row in dataset:\n",
    "  response = graph.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
    "print(\"RAG chain executed successfully.\")\n",
    "\n",
    "# RAG evaluation\n",
    "print(\"Executing RAGAS evaluation...\")\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
    "baseline_evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "baseline_ragas_report = evaluate(\n",
    "    dataset=baseline_evaluation_dataset,\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        answer_correctness\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "\n",
    "print(\"RAGAS Results using Naive Retrieval:\")\n",
    "print(baseline_ragas_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ce555",
   "metadata": {},
   "source": [
    "## **Step 3: LangGraph RAG with Semantic Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c3d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of chunks: 211\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import re\n",
    "import time\n",
    "\n",
    "# ================================\n",
    "# Step 1: Semantic Chunking Logic\n",
    "# ================================\n",
    "semantic_embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def clean_sentences(sentences: list[str], min_words: int = 3) -> list[str]:\n",
    "    cleaned = []\n",
    "    for s in sentences:\n",
    "        # Replace newlines, non-breaking spaces, HTML entities\n",
    "        s = s.replace(\"\\n\\n\", \" \").replace(\"\\xa0\", \" \").replace(\"&nbsp;\", \" \")\n",
    "        s = re.sub(r\"\\s+\", \" \", s)  # Collapse any repeated whitespace\n",
    "        s = s.strip()\n",
    "\n",
    "        # Skip empty or too-short sentences\n",
    "        if len(s.split()) >= min_words:\n",
    "            cleaned.append(s)\n",
    "    return cleaned\n",
    "\n",
    "def get_embeddings(sentences: list[str]) -> list[np.ndarray]:\n",
    "    while True:\n",
    "        try:\n",
    "            return semantic_embedding_model.embed_documents(sentences)\n",
    "        except Exception as e:\n",
    "            print(f\"Retrying due to error: {e}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "def semantic_chunk(sentences, threshold=0.85, max_sentences=15, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    Adjusted semantic chunking logic to reduce the number of granular chunks.\n",
    "    \n",
    "    Parameters:\n",
    "    - sentences: List of sentences to be chunked.\n",
    "    - threshold: Similarity threshold for including a sentence in the current chunk.\n",
    "    - max_sentences: Maximum number of sentences in a chunk.\n",
    "    - max_tokens: Maximum number of tokens in a chunk.\n",
    "    \n",
    "    Returns:\n",
    "    - List of text chunks.\n",
    "    \"\"\"\n",
    "    # Clean the input sentences\n",
    "    sentences = clean_sentences(sentences)\n",
    "\n",
    "    # Embed cleaned sentences\n",
    "    embeddings = get_embeddings(sentences)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_vectors = [embeddings[0]]\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        proposed_chunk = \" \".join(current_chunk + [sentences[i]])\n",
    "        total_tokens = count_tokens(proposed_chunk)\n",
    "\n",
    "        current_avg = np.mean(current_vectors, axis=0)\n",
    "        sim = cosine_similarity([embeddings[i]], [current_avg])[0][0]\n",
    "\n",
    "        if sim > threshold and (len(current_chunk) < max_sentences or total_tokens <= max_tokens):\n",
    "            current_chunk.append(sentences[i])\n",
    "            current_vectors.append(embeddings[i])\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "            current_vectors = [embeddings[i]]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ================================\n",
    "# Step 2: Load and Preprocess HTML\n",
    "# ================================\n",
    "path = \"data/\"\n",
    "loader = DirectoryLoader(path, glob=\"*.html\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Extract text and break into semantic chunks\n",
    "all_chunks = []\n",
    "for doc in docs:\n",
    "    # Break raw content into sentences (basic split, can improve)\n",
    "    raw_text = doc.page_content\n",
    "    sentences = re.split(r'(?<=[.!?]) +', raw_text.strip())\n",
    "    chunks = semantic_chunk(sentences)\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "\n",
    "print(f\"# of chunks: {len(all_chunks)}\")\n",
    "\n",
    "# ================================\n",
    "# Step 3: Vector DB Setup (Qdrant)\n",
    "# ================================\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "qdrant = QdrantClient(\":memory:\")\n",
    "collection_name = \"semantic_chunking\"\n",
    "\n",
    "qdrant.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=qdrant,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "_ = vectorstore.add_documents(documents=all_chunks)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "\n",
    "# ================================\n",
    "# Step 4: LangGraph State Definition\n",
    "# ================================\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "# ================================\n",
    "# Step 5: Nodes (Retrieve + Generate)\n",
    "# ================================\n",
    "def retrieve(state):\n",
    "    retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "\n",
    "def generate(state):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = rag_prompt.format_messages(\n",
    "        question=state[\"question\"],\n",
    "        context=docs_content\n",
    "    )\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "# ================================\n",
    "# Step 6: Build LangGraph\n",
    "# ================================\n",
    "graph_builder = StateGraph(GraphState).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ace7c0",
   "metadata": {},
   "source": [
    "## **Step 4: Semantic Chunking Evaluation using RAGAS Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4f8ba9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b44b178e1634c4ea45cd99bda304e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37ba5bdc22c408893d0804d5f4c8c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bcc8898a014c3d9ec15aeb8f6e69ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86be0a9ba55440c193ae81b1b1d3278b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5239729b1a1540e1878d4bf4cd890dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8835350be8a0470780177c0b1a07b2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc30a52924b4dacb4e5c9ef60200537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb2df178f834d848ad92bc3d886c9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e302bea379684b968dbd05bdd0021459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generated.\n",
      "Executing RAG chain...\n",
      "RAG chain executed successfully.\n",
      "Executing RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779395ae27cc493f8831a9e6b6ff0ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[14]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29545, Requested 2085. Please try again in 3.26s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[10]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29927, Requested 2767. Please try again in 5.388s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[24]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29862, Requested 1900. Please try again in 3.524s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[23]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 30000, Requested 1787. Please try again in 3.574s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[20]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29199, Requested 1900. Please try again in 2.198s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[29]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29665, Requested 2221. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[4]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 29392, Requested 2250. Please try again in 3.284s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[30]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 28307, Requested 2215. Please try again in 1.044s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[35]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-ULKD0BfHsBV4junY2FxMu8EA on tokens per min (TPM): Limit 30000, Used 27949, Requested 2171. Please try again in 240ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS Results using Semantic Chunking:\n",
      "{'faithfulness': 0.5406, 'answer_relevancy': 0.9578, 'context_precision': 0.7441, 'context_recall': 0.5790, 'answer_correctness': 0.6453}\n"
     ]
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import EvaluationDataset\n",
    "from ragas import evaluate, RunConfig\n",
    "from ragas.metrics import (\n",
    "    faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness\n",
    ")\n",
    "\n",
    "# Generate testset from source documents\n",
    "print(\"Generating dataset...\")\n",
    "generator_llm = LangchainLLMWrapper(langchain_llm=ChatOpenAI(model=\"gpt-4.1\"),)\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(embeddings=embeddings)\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)\n",
    "print(\"Dataset generated.\")\n",
    "\n",
    "\n",
    "# Run inference and update answers\n",
    "print(\"Executing RAG chain...\")\n",
    "for test_row in dataset:\n",
    "  response = graph.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
    "print(\"RAG chain executed successfully.\")\n",
    "\n",
    "# RAG evaluation\n",
    "print(\"Executing RAGAS evaluation...\")\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
    "semantic_chunk_evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "semantic_chunk_ragas_report = evaluate(\n",
    "    dataset=semantic_chunk_evaluation_dataset,\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        answer_correctness\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "\n",
    "print(\"RAGAS Results using Semantic Chunking:\")\n",
    "print(semantic_chunk_ragas_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9138ecc",
   "metadata": {},
   "source": [
    "## **Step 5: Comparison between Naive Chunking and Semantic Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "80c22899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Naive Chunking",
         "text": [
          "0.87",
          "0.79",
          "0.53",
          "0.87",
          "0.77"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "faithfulness",
          "answer_relevancy",
          "context_precision",
          "context_recall",
          "answer_correctness"
         ],
         "y": [
          0.8681167203723594,
          0.7867835089976833,
          0.5333333333024538,
          0.8701499118165785,
          0.7672635896248067
         ]
        },
        {
         "name": "Semantic Chunking",
         "text": [
          "0.54",
          "0.96",
          "0.74",
          "0.58",
          "0.65"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "faithfulness",
          "answer_relevancy",
          "context_precision",
          "context_recall",
          "answer_correctness"
         ],
         "y": [
          0.540602453102453,
          0.9578365610274631,
          0.7440972221684462,
          0.5790043290043291,
          0.6453190010307048
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "RAGAS Metric Comparison (Averaged): Naive vs Semantic Chunking"
        },
        "yaxis": {
         "range": [
          0,
          1
         ],
         "title": {
          "text": "Score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Convert RAGAS reports to DataFrames\n",
    "naive_scores_df = baseline_ragas_report.to_pandas()\n",
    "semantic_scores_df = semantic_chunk_ragas_report.to_pandas()\n",
    "\n",
    "# Metrics to include\n",
    "selected_metrics = [\n",
    "    \"faithfulness\",\n",
    "    \"answer_relevancy\",\n",
    "    \"context_precision\",\n",
    "    \"context_recall\",\n",
    "    \"answer_correctness\"\n",
    "]\n",
    "\n",
    "# Compute average scores for each metric\n",
    "naive_values = naive_scores_df[selected_metrics].mean().tolist()\n",
    "semantic_values = semantic_scores_df[selected_metrics].mean().tolist()\n",
    "\n",
    "# Create grouped bar chart with values displayed\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        name='Naive Chunking',\n",
    "        x=selected_metrics,\n",
    "        y=naive_values,\n",
    "        text=[f\"{v:.2f}\" for v in naive_values],\n",
    "        textposition=\"auto\"\n",
    "    ),\n",
    "    go.Bar(\n",
    "        name='Semantic Chunking',\n",
    "        x=selected_metrics,\n",
    "        y=semantic_values,\n",
    "        text=[f\"{v:.2f}\" for v in semantic_values],\n",
    "        textposition=\"auto\"\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='RAGAS Metric Comparison (Averaged): Naive vs Semantic Chunking',\n",
    "    yaxis=dict(title='Score', range=[0, 1]),\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112260e7",
   "metadata": {},
   "source": [
    "| Metric                 | Naive Chunking | Semantic Chunking | Higher Is Better | Insight                                                                 |\n",
    "| ---------------------- | -------------- | ----------------- | ---------------- | ----------------------------------------------------------------------- |\n",
    "| **Faithfulness**       | 0.87           | 0.54              | ✅ Naive          | Naive chunking provided more faithful responses with respect to sources |\n",
    "| **Answer Relevancy**   | 0.79           | 0.96              | ✅ Semantic       | Semantic chunking significantly improved relevance of answers           |\n",
    "| **Context Precision**  | 0.53           | 0.74              | ✅ Semantic       | Semantic chunking led to tighter, more precise retrieval                |\n",
    "| **Context Recall**     | 0.87           | 0.58              | ✅ Naive          | Naive chunking covered more relevant context overall                    |\n",
    "| **Answer Correctness** | 0.77           | 0.65              | ✅ Naive          | Naive chunking slightly outperformed in correctness of final answers    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de97e0a4",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "* **Semantic chunking improved**:\n",
    "\n",
    "  * *Answer relevancy* and *context precision*, likely due to more focused retrieval from semantically similar text groups.\n",
    "\n",
    "* **Naive chunking was stronger** in:\n",
    "\n",
    "  * *Faithfulness*, *context recall*, and *answer correctness*, suggesting that longer or overlapping chunks may better preserve full context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
